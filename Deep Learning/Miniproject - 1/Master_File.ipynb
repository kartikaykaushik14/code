{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master File.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "vTJVHn5ZKB5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twCSCEWuJ0ZD"
      },
      "outputs": [],
      "source": [
        "para_sum = 0\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        \n",
        "        \n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        \n",
        "        global para_sum\n",
        "        print(\"_____BasicBlock parameters \" + str(3*3*planes*in_planes)) #testprint\n",
        "        para_sum += 3*3*planes*in_planes #parameter sum\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        \n",
        "        print(\"_____BasicBlock parameters \" + str(3*3*planes*planes)) #testprint\n",
        "        para_sum += 3*3*planes*planes #parameter sum\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "            print(\"_____SkipBlock parameters \" + str(1*1*in_planes*planes)) #testprint\n",
        "            para_sum += 1*1*planes*in_planes #parameter sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        out = self.drop(out)\n",
        "\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_planes = 64\n",
        "        print(\"Input Channels \" + str(self.in_planes))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        \n",
        "        global para_sum\n",
        "        print(\"_____InputLayer parameters \" + str(3*3*self.in_planes*3)) \n",
        "        para_sum += 3*3*self.in_planes*3 \n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "\n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "\n",
        "        print(\"_____LinearLayer parameters \" + str(2048*num_classes)) \n",
        "        para_sum += (2048*num_classes) \n",
        "        print(\"_____TOTAL PARASUM  \" + str(para_sum))\n",
        "\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "wCVKgwOJLRGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0,0,0], [1,1,1])\n",
        "])\n",
        "\n",
        "orgtrainingdata = torchvision.datasets.CIFAR10(root = 'data/', train = True, download = True, transform = normalize_transform)\n",
        "\n",
        "train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomCrop(32, padding=4),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0,0,0], [1,1,1], inplace=True)\n",
        "])\n",
        "\n",
        "\n",
        "modtrainingdata = torchvision.datasets.CIFAR10(root = 'data/', train = True, download = True, transform = train_transform)\n",
        "\n",
        "trainingdata = torch.utils.data.ConcatDataset([modtrainingdata, orgtrainingdata])\n",
        "\n",
        "testdata = torchvision.datasets.CIFAR10(root = 'data/', train = False, download = True, transform = normalize_transform)\n",
        "print(len(testdata))\n",
        "print(len(trainingdata))\n",
        "\n",
        "trainDataLoader = torch.utils.data.DataLoader(trainingdata, batch_size=128, shuffle=True)\n",
        "testDataLoader = torch.utils.data.DataLoader(testdata, batch_size=128, shuffle=False )\n",
        "\n",
        "myConfig = [8, 8]\n",
        "print(\"Layers: \" + str(myConfig))\n",
        "model = ResNet(BasicBlock, myConfig).cuda()\n",
        "Loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "LR = 0.001\n",
        "print(\"Learning Rate = \" + str(LR))\n",
        "WD = 0.0001\n",
        "print(\"Weight Decay = \" + str(WD))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "para_sum = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TtKFxNPVKPzW",
        "outputId": "2e48f12c-c1f9-4b8d-e1d1-d01f540c13b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "10000\n",
            "100000\n",
            "Layers: [8, 8]\n",
            "Input Channels 64\n",
            "_____InputLayer parameters 1728\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 36864\n",
            "_____BasicBlock parameters 73728\n",
            "_____BasicBlock parameters 147456\n",
            "_____SkipBlock parameters 8192\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____BasicBlock parameters 147456\n",
            "_____LinearLayer parameters 20480\n",
            "_____TOTAL PARASUM  2905792\n",
            "Learning Rate = 0.001\n",
            "Weight Decay = 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "epochs = 50\n",
        "max_val = 0\n",
        "max_idx = 0\n",
        "\n",
        "for epoch in range (epochs):\n",
        "  train_loss = 0.0\n",
        "  test_loss = 0.0\n",
        "  test_accuracy = 0.0\n",
        "\n",
        "  for i, data in enumerate(trainDataLoader):\n",
        "    images, labels = data\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    predicted_output = model(images)\n",
        "    fit = Loss(predicted_output,labels)\n",
        "    fit.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += fit.item()\n",
        "  \n",
        "  for i, data in enumerate(testDataLoader):\n",
        "    with torch.no_grad():\n",
        "      images, labels = data\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      predicted_output = model(images)\n",
        "      fit = Loss(predicted_output, labels)\n",
        "      test_loss += fit.item()\n",
        "      test_accuracy += (torch.eq(torch.max(predicted_output, 1)[1],labels).sum()).data.cpu().numpy()\n",
        "\n",
        "  train_loss = train_loss/len(trainDataLoader)\n",
        "  test_loss = test_loss/len(testDataLoader)\n",
        "  test_accuracy = 100 * (test_accuracy/10000)\n",
        "\n",
        "  if (test_accuracy > max_val):\n",
        "    max_val = test_accuracy\n",
        "    max_idx = epoch\n",
        "  \n",
        "  print('Epoch %d, Train loss %.4f, Test loss %.4f, test_accuracy %.4f'%(epoch, train_loss, test_loss, test_accuracy))\n",
        "\n",
        "  train_loss_history.append(train_loss)\n",
        "  test_loss_history.append(test_loss)\n",
        "\n",
        "print(\"________________________\")\n",
        "print(\"Max accuracy = %.4f in epoch = %d\"%(max_val, max_idx))\n",
        "print(\"_________Training Loss_______________\")\n",
        "print(train_loss_history)\n",
        "print(\"_________Testing Loss_______________\")\n",
        "print(test_loss_history)\n",
        "\n",
        "import os\n",
        "PATH = os.getcwd();\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "2Nr43nI_K_J1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
